{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36024c72-6afd-429c-914e-0b6ed5a03b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "440167e4-7914-430f-8d03-6c28187a92f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = torch.zeros((5, 3, 1024, 1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd5d7839-cd6b-4589-9368-29cb175e6d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from GNN.src.dnn_f import DNN_F\n",
    "model = DNN_F()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03c2e2b6-0105-41cd-888b-83751965b48f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(image).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3ab13746-5645-412f-b947-92b6f6564a0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test_Both.pkl             Test_VEGF.pkl             Train_VEGF_normalized.pkl\n",
      "Test_Both_normalized.pkl  Test_VEGF_normalized.pkl  Valid_Both_normalized.pkl\n",
      "Test_TER.pkl              Train_Both_normalized.pkl Valid_TER_normalized.pkl\n",
      "Test_TER_normalized.pkl   Train_TER_normalized.pkl  Valid_VEGF_normalized.pkl\n"
     ]
    }
   ],
   "source": [
    "!ls \"../Data/FinalSplits/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "48f4b727-ca65-4c79-8705-aadc10c207c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from tqdm import tqdm\n",
    "\n",
    "import math\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "def check_for_nan(dataset):\n",
    "    for i, data in enumerate(dataset):\n",
    "        if torch.isnan(data.x).any():\n",
    "            print(f\"NaN found in features at index {i}\")\n",
    "        if torch.isnan(data.y).any():\n",
    "            print(f\"NaN found in target at index {i}\")\n",
    "\n",
    "def load_dataset_from_pickle(pickle_file):\n",
    "    with open(pickle_file, 'rb') as f:\n",
    "        dataset = pickle.load(f)\n",
    "    if isinstance(dataset, list) and all(isinstance(d, Data) for d in dataset):\n",
    "        return dataset\n",
    "    else:\n",
    "        raise ValueError(\"The loaded dataset is not a list of Data objects).\")\n",
    "    \n",
    "def print_stats(train_dataset, val_dataset, test_dataset, num_features, num_targets, print_detailed = False):\n",
    "    print(\"###################################\")\n",
    "    print()\n",
    "    print(f'Number of training graphs: {len(train_dataset)}')\n",
    "    print(f'Number of validation graphs: {len(val_dataset)}')\n",
    "    print(f'Number of test graphs: {len(test_dataset)}')\n",
    "    print(f'Number of features: {num_features}')\n",
    "    print(f'Number of targets: {num_targets}')\n",
    "    print(\"###################################\")\n",
    "\n",
    "    if print_detailed:\n",
    "        data = train_dataset[0]\n",
    "        print()\n",
    "        print(data)\n",
    "        print('=============================================================')\n",
    "        print(f'Number of nodes: {data.num_nodes}')\n",
    "        print(f'Number of edges: {data.num_edges}')\n",
    "        print(f'Average node degree: {data.num_edges / data.num_nodes:.2f}')\n",
    "        print(f'Has isolated nodes: {data.has_isolated_nodes()}')\n",
    "        print(f'Has self-loops: {data.has_self_loops()}')\n",
    "        print(f'Is undirected: {data.is_undirected()}')\n",
    "        print('=============================================================')\n",
    "        print()\n",
    "        print(f'Number of training graphs: {len(train_dataset)}')\n",
    "        print(f'Number of test graphs: {len(val_dataset)}')\n",
    "        print('=============================================================')\n",
    "        print()\n",
    "        # for step, data in enumerate(train_loader):\n",
    "        #     print(f'Step {step + 1}:')\n",
    "        #     print('=======')\n",
    "        #     print(f'Number of graphs in the current batch: {data.num_graphs}')\n",
    "        #     print(data)\n",
    "        #     print()\n",
    "        print()\n",
    "\n",
    "def get_loaders(data_dirs, target, batch_size, print_data_stats = True, print_detailed = False):\n",
    "    train_pickle_file = data_dirs[f\"Train_{target}\"]\n",
    "    val_pickle_file = data_dirs[f\"Valid_{target}\"]\n",
    "    test_pickle_file = data_dirs[f\"Test_{target}\"]\n",
    "\n",
    "    # train_dataset = load_dataset_from_pickle(train_pickle_file)\n",
    "    # val_dataset = load_dataset_from_pickle(val_pickle_file)\n",
    "    test_dataset = load_dataset_from_pickle(test_pickle_file)\n",
    "\n",
    "    # check_for_nan(train_dataset)\n",
    "    # check_for_nan(val_dataset)\n",
    "    check_for_nan(test_dataset)\n",
    "\n",
    "    num_features = test_dataset[0].x.shape[1]  # Number of features per node\n",
    "    num_targets = test_dataset[0].y.shape[0]\n",
    "\n",
    "    detail_list = [num_features, num_targets]\n",
    "    \n",
    "    if print_data_stats: print_stats(train_dataset, val_dataset, test_dataset, num_features, num_targets, print_detailed)\n",
    "\n",
    "    # train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    # val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return test_loader, detail_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b244b4d2-e73b-4ab1-9d1f-5efff4dda0a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/qbam_gnn/lib/python3.12/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(io.BytesIO(b))\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'train_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m     data_dirs[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValid_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/Valid_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mnorm_string\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      8\u001b[0m     data_dirs[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/Test_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mnorm_string\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 10\u001b[0m test_loader, data_details \u001b[38;5;241m=\u001b[39m \u001b[43mget_loaders\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_dirs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[16], line 78\u001b[0m, in \u001b[0;36mget_loaders\u001b[0;34m(data_dirs, target, batch_size, print_data_stats, print_detailed)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# check_for_nan(train_dataset)\u001b[39;00m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;66;03m# check_for_nan(val_dataset)\u001b[39;00m\n\u001b[1;32m     76\u001b[0m check_for_nan(test_dataset)\n\u001b[0;32m---> 78\u001b[0m num_features \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_dataset\u001b[49m[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mx\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]  \u001b[38;5;66;03m# Number of features per node\u001b[39;00m\n\u001b[1;32m     79\u001b[0m num_targets \u001b[38;5;241m=\u001b[39m train_dataset[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39my\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     81\u001b[0m detail_list \u001b[38;5;241m=\u001b[39m [num_features, num_targets]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "data=\"../Data/FinalSplits\"\n",
    "norm_string=\"\"\n",
    "target=\"TER\"\n",
    "data_dirs = {}\n",
    "for data_type in ['TER', 'VEGF', 'Both']:\n",
    "    data_dirs[f\"Train_{data_type}\"] = f\"{data}/Train_{data_type}{norm_string}.pkl\"\n",
    "    data_dirs[f\"Valid_{data_type}\"] = f\"{data}/Valid_{data_type}{norm_string}.pkl\"\n",
    "    data_dirs[f\"Test_{data_type}\"] = f\"{data}/Test_{data_type}{norm_string}.pkl\"\n",
    "\n",
    "test_loader, data_details = get_loaders(data_dirs, target, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8bfb938-7667-46e1-a028-6cdf547484bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import GNN.src.gnn_multiple as GCNs\n",
    "model_constructors = {\n",
    "    \"G2_D2\": GCNs.GCN_G2_D2,\n",
    "    \"G2_D3\": GCNs.GCN_G2_D3,\n",
    "    \"G2_D4\": GCNs.GCN_G2_D4,\n",
    "    \"G2_D5\": GCNs.GCN_G2_D5,\n",
    "    \"G3_D2\": GCNs.GCN_G3_D2,\n",
    "    \"G3_D3\": GCNs.GCN_G3_D3,\n",
    "    \"G3_D4\": GCNs.GCN_G3_D4,\n",
    "    \"G3_D5\": GCNs.GCN_G3_D5,\n",
    "    \"G4_D2\": GCNs.GCN_G4_D2,\n",
    "    \"G4_D3\": GCNs.GCN_G4_D3,\n",
    "    \"G4_D4\": GCNs.GCN_G4_D4,\n",
    "    \"G4_D5\": GCNs.GCN_G4_D5,\n",
    "    \"G5_D2\": GCNs.GCN_G5_D2,\n",
    "    \"G5_D3\": GCNs.GCN_G5_D3,\n",
    "    \"G5_D4\": GCNs.GCN_G5_D4,\n",
    "    \"G5_D5\": GCNs.GCN_G5_D5\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3885881a-c2ad-4318-aa91-107a0ca60086",
   "metadata": {},
   "outputs": [],
   "source": [
    "from GNN.src.gnn_modular import Modular_GCN\n",
    "modular_gcn = Modular_GCN"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "QBAM GNN",
   "language": "python",
   "name": "qbam_gnn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
